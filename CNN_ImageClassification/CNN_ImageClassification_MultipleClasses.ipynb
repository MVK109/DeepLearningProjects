{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d1a6d3",
   "metadata": {},
   "source": [
    "# Data Path https://data.caltech.edu/records/mzrjq-6wc02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd40a03",
   "metadata": {},
   "source": [
    "#  Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f96b2af-1d2e-4fd2-8c9f-2e8cad495b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout,BatchNormalization\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec5758",
   "metadata": {},
   "source": [
    "# Initialising the CNN with 3 convolutional layers, filters, feature maps, fullyconnected layers,using drop out to prevent overfitting,padding,maxpooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a7ee36-5187-47e2-aeb4-19867516b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32,kernel_size=(3, 3), padding='same',input_shape = (300, 300, 3), activation = 'relu'))\n",
    "#classifier.add(BatchNormalization())\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32,kernel_size=(3, 3),padding='same', activation = 'relu'))\n",
    "#classifier.add(BatchNormalization())\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a third convolutional layer\n",
    "\n",
    "classifier.add(Conv2D(32,kernel_size=(3, 3),padding='same', activation = 'relu'))\n",
    "#classifier.add(BatchNormalization())\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# # Adding a fourth convolutional layer\n",
    "\n",
    "\n",
    "# classifier.add(Conv2D(32,kernel_size=(3, 3),padding='same', activation = 'relu'))\n",
    "# #classifier.add(BatchNormalization())\n",
    "\n",
    "# classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "Dropout(0.5)\n",
    "classifier.add(Dense(units = 64, activation = 'relu'))\n",
    "Dropout(0.5)\n",
    "classifier.add(Dense(units = 32, activation = 'relu'))\n",
    "Dropout(0.5)\n",
    "classifier.add(Dense(5, activation = 'Softmax'))\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), patience=5)\n",
    "\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb5c4b5",
   "metadata": {},
   "source": [
    "# Importing ImageDataGenerator which helps us to normalize the data by bringing the data into range(0,1),setting batch size, target_size and requidred class_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57dcf377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96 images belonging to 5 classes.\n",
      "Found 98 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    r'D:\\QTInternship\\caltech-101\\caltech-101\\training_data_multiclass_6',\n",
    "    target_size=(300, 300),\n",
    "    batch_size=100,\n",
    "    class_mode='categorical'  # For multiclass classification\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    r'D:\\QTInternship\\caltech-101\\caltech-101\\testing_data_multiclass_6',  # Ensure you have the correct path\n",
    "    target_size=(300, 300),\n",
    "    batch_size=100,\n",
    "    class_mode='categorical'  # For multiclass classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d1ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), patience=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db2c53",
   "metadata": {},
   "source": [
    "# Fitting the model by specifying epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db028ad-f0a9-40a6-be03-e39fcce16537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.6096 - accuracy: 0.2500 - val_loss: 4.7813 - val_accuracy: 0.3265\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 4.2667 - accuracy: 0.2708 - val_loss: 1.8387 - val_accuracy: 0.2041\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.7457 - accuracy: 0.2708 - val_loss: 1.5851 - val_accuracy: 0.2245\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.5237 - accuracy: 0.3333 - val_loss: 1.5624 - val_accuracy: 0.2755\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.4959 - accuracy: 0.4479 - val_loss: 1.5180 - val_accuracy: 0.3776\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.4132 - accuracy: 0.5521 - val_loss: 1.5224 - val_accuracy: 0.3878\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.3492 - accuracy: 0.5208 - val_loss: 1.5301 - val_accuracy: 0.3673\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.2714 - accuracy: 0.4688 - val_loss: 1.4925 - val_accuracy: 0.3980\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.1906 - accuracy: 0.5417 - val_loss: 1.5268 - val_accuracy: 0.5102\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.1104 - accuracy: 0.6354 - val_loss: 1.4841 - val_accuracy: 0.4286\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.0183 - accuracy: 0.6250 - val_loss: 1.6027 - val_accuracy: 0.5204\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.9687 - accuracy: 0.6562 - val_loss: 1.4907 - val_accuracy: 0.4388\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.8989 - accuracy: 0.6146 - val_loss: 1.5748 - val_accuracy: 0.5918\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.7666 - accuracy: 0.7604 - val_loss: 1.5537 - val_accuracy: 0.7143\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.7457 - accuracy: 0.8438 - val_loss: 1.4875 - val_accuracy: 0.6429\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6719 - accuracy: 0.8542 - val_loss: 1.5264 - val_accuracy: 0.6939\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6390 - accuracy: 0.8125 - val_loss: 1.7230 - val_accuracy: 0.6939\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.5766 - accuracy: 0.8542 - val_loss: 1.9603 - val_accuracy: 0.6735\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.5024 - accuracy: 0.8646 - val_loss: 1.8938 - val_accuracy: 0.6939\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.4816 - accuracy: 0.8333 - val_loss: 1.9906 - val_accuracy: 0.6837\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.4482 - accuracy: 0.8542 - val_loss: 1.9894 - val_accuracy: 0.7041\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.3443 - accuracy: 0.8958 - val_loss: 1.9850 - val_accuracy: 0.7041\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.3371 - accuracy: 0.8958 - val_loss: 1.9988 - val_accuracy: 0.7143\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.2587 - accuracy: 0.9271 - val_loss: 2.0112 - val_accuracy: 0.6837\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.2440 - accuracy: 0.9167 - val_loss: 2.0841 - val_accuracy: 0.6837\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1806 - accuracy: 0.9375 - val_loss: 2.2891 - val_accuracy: 0.6633\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1576 - accuracy: 0.9688 - val_loss: 2.0632 - val_accuracy: 0.7041\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.2207 - accuracy: 0.8958 - val_loss: 2.4074 - val_accuracy: 0.7041\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1996 - accuracy: 0.9271 - val_loss: 2.1259 - val_accuracy: 0.7041\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0989 - accuracy: 0.9792 - val_loss: 2.4388 - val_accuracy: 0.6633\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1249 - accuracy: 0.9688 - val_loss: 2.6372 - val_accuracy: 0.7245\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0853 - accuracy: 0.9896 - val_loss: 2.8744 - val_accuracy: 0.7551\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1227 - accuracy: 0.9479 - val_loss: 2.2111 - val_accuracy: 0.7245\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0756 - accuracy: 0.9688 - val_loss: 2.0044 - val_accuracy: 0.7245\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0658 - accuracy: 0.9792 - val_loss: 1.8059 - val_accuracy: 0.7449\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0444 - accuracy: 1.0000 - val_loss: 1.7331 - val_accuracy: 0.7551\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.0415 - accuracy: 0.9896 - val_loss: 1.9584 - val_accuracy: 0.7347\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 2.2592 - val_accuracy: 0.7245\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0388 - accuracy: 0.9896 - val_loss: 2.5440 - val_accuracy: 0.7245\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 2.6685 - val_accuracy: 0.7449\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 2.6616 - val_accuracy: 0.7449\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0621 - accuracy: 0.9896 - val_loss: 2.3830 - val_accuracy: 0.7551\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0227 - accuracy: 0.9896 - val_loss: 2.4044 - val_accuracy: 0.7449\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0964 - accuracy: 0.9792 - val_loss: 1.7869 - val_accuracy: 0.8163\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 2.0898 - val_accuracy: 0.7653\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1247 - accuracy: 0.9479 - val_loss: 1.9911 - val_accuracy: 0.7755\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 2.4552 - val_accuracy: 0.7449\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0904 - accuracy: 0.9792 - val_loss: 2.7136 - val_accuracy: 0.7245\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1647 - accuracy: 0.9792 - val_loss: 2.1466 - val_accuracy: 0.7653\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 2.5478 - val_accuracy: 0.7245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x276c164e820>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate steps per epoch\n",
    "# steps_per_epoch = training_set.samples // training_set.batch_size\n",
    "# print(steps_per_epoch)\n",
    "# validation_steps = test_set.samples // test_set.batch_size\n",
    "# print(validation_steps)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(\n",
    "    training_set,\n",
    "#     steps_per_epoch=50,\n",
    "    epochs=50,\n",
    "    validation_data=test_set\n",
    "#     validation_steps=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06d94a",
   "metadata": {},
   "source": [
    "# Testing our models performance by selecting specific images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0afff5d4-eeb0-46f6-a55d-8cdcab9eebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 191ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "#from keras.preprocessing import image\n",
    "test_image = image.load_img('D:/QTInternship/caltech-101/caltech-101/101_ObjectCategories/airplanes/image_0164.jpg'\n",
    "                   , target_size=(300, 300))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56061abf-ef38-4131-bf95-71244cb92209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30aacd67-0064-4a3b-a030-806b6e5a2e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/101_ObjectCategories/butterfly/image_0016.jpg'\n",
    "                   , target_size=(300, 300))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3974e5ff-a483-4fbf-8e82-7c7dd271c1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af7bea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/101_ObjectCategories/dragonfly/image_0026.jpg'\n",
    "#                    , target_size=(300, 300))\n",
    "# test_image = image.img_to_array(test_image)\n",
    "# test_image = np.expand_dims(test_image, axis=0)\n",
    "# result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f55775b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/101_ObjectCategories/dragonfly/image_0030.jpg'\n",
    "#                    , target_size=(300, 300))\n",
    "# test_image = image.img_to_array(test_image)\n",
    "# test_image = np.expand_dims(test_image, axis=0)\n",
    "# result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dfdc95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/101_ObjectCategories/car_side/image_0045.jpg'\n",
    "                   , target_size=(300, 300))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50879d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000e+00 2.40614e-10 1.00000e+00 0.00000e+00 0.00000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "261cdb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/101_ObjectCategories/cellphone/image_0034.jpg'\n",
    "                   , target_size=(300, 300))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37821e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0bd7aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/101_ObjectCategories/cougar_body/image_0029.jpg'\n",
    "                   , target_size=(300, 300))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcbb214b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a512ff",
   "metadata": {},
   "source": [
    "# Data Labels used for ImageClassification\n",
    "\n",
    "## airplanes\n",
    "## butterfly\n",
    "## car_side\n",
    "## cellphone\n",
    "## cougar_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5457fe2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
