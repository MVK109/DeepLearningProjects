{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d1a6d3",
   "metadata": {},
   "source": [
    "# Data Path https://data.caltech.edu/records/mzrjq-6wc02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd40a03",
   "metadata": {},
   "source": [
    "#  Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f96b2af-1d2e-4fd2-8c9f-2e8cad495b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout,BatchNormalization\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec5758",
   "metadata": {},
   "source": [
    "# Initialising the CNN with 3 convolutional layers, filters, feature maps, fullyconnected layers,using drop out to prevent overfitting,padding,maxpooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a7ee36-5187-47e2-aeb4-19867516b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32,kernel_size=(3, 3), padding='same',input_shape = (128, 128, 3), activation = 'relu'))\n",
    "#classifier.add(BatchNormalization())\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32,kernel_size=(3, 3),padding='same', activation = 'relu'))\n",
    "#classifier.add(BatchNormalization())\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# # Adding a third convolutional layer\n",
    "\n",
    "# classifier.add(Conv2D(32,kernel_size=(3, 3),padding='same', activation = 'relu'))\n",
    "# #classifier.add(BatchNormalization())\n",
    "\n",
    "# classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# # Adding a fourth convolutional layer\n",
    "\n",
    "\n",
    "# classifier.add(Conv2D(32,kernel_size=(3, 3),padding='same', activation = 'relu'))\n",
    "# #classifier.add(BatchNormalization())\n",
    "\n",
    "# classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "Dropout(0.3)\n",
    "# classifier.add(Dense(units = 64, activation = 'relu'))\n",
    "# Dropout(0.3)\n",
    "# classifier.add(Dense(units = 32, activation = 'relu'))\n",
    "# Dropout(0.3)\n",
    "classifier.add(Dense(8, activation = 'Softmax'))\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), patience=5)\n",
    "\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb5c4b5",
   "metadata": {},
   "source": [
    "# Importing ImageDataGenerator which helps us to normalize the data by bringing the data into range(0,1),setting batch size, target_size and requidred class_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57dcf377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 970 images belonging to 8 classes.\n",
      "Found 246 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    r'D:\\QTInternship\\caltech-101\\caltech-101\\traindata',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=128,\n",
    "    class_mode='categorical'  # For multiclass classification\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    r'D:\\QTInternship\\caltech-101\\caltech-101\\testdata',  # Ensure you have the correct path\n",
    "    target_size=(128, 128),\n",
    "    batch_size=128,\n",
    "    class_mode='categorical'  # For multiclass classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10d1ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), patience=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db2c53",
   "metadata": {},
   "source": [
    "# Fitting the model by specifying epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db028ad-f0a9-40a6-be03-e39fcce16537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.7668 - accuracy: 0.5433 - val_loss: 1.2408 - val_accuracy: 0.6504\n",
      "Epoch 2/40\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.2106 - accuracy: 0.6691 - val_loss: 1.0883 - val_accuracy: 0.6829\n",
      "Epoch 3/40\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.0612 - accuracy: 0.6887 - val_loss: 0.9945 - val_accuracy: 0.6992\n",
      "Epoch 4/40\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.9503 - accuracy: 0.7031 - val_loss: 0.8289 - val_accuracy: 0.7073\n",
      "Epoch 5/40\n",
      "8/8 [==============================] - 8s 1s/step - loss: 0.8208 - accuracy: 0.7402 - val_loss: 0.7411 - val_accuracy: 0.7439\n",
      "Epoch 6/40\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.7204 - accuracy: 0.7649 - val_loss: 0.6324 - val_accuracy: 0.7724\n",
      "Epoch 7/40\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.6400 - accuracy: 0.7804 - val_loss: 0.6162 - val_accuracy: 0.7724\n",
      "Epoch 8/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.6270 - accuracy: 0.7990 - val_loss: 0.5641 - val_accuracy: 0.7805\n",
      "Epoch 9/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.5703 - accuracy: 0.8093 - val_loss: 0.5376 - val_accuracy: 0.8008\n",
      "Epoch 10/40\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.4789 - accuracy: 0.8392 - val_loss: 0.4616 - val_accuracy: 0.8252\n",
      "Epoch 11/40\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.4146 - accuracy: 0.8598 - val_loss: 0.4497 - val_accuracy: 0.8333\n",
      "Epoch 12/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.3593 - accuracy: 0.8794 - val_loss: 0.4809 - val_accuracy: 0.8252\n",
      "Epoch 13/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.3137 - accuracy: 0.8959 - val_loss: 0.4706 - val_accuracy: 0.8455\n",
      "Epoch 14/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.2877 - accuracy: 0.9072 - val_loss: 0.5592 - val_accuracy: 0.8252\n",
      "Epoch 15/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.3098 - accuracy: 0.9072 - val_loss: 0.4970 - val_accuracy: 0.8374\n",
      "Epoch 16/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.2808 - accuracy: 0.9082 - val_loss: 0.4967 - val_accuracy: 0.8293\n",
      "Epoch 17/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.2142 - accuracy: 0.9392 - val_loss: 0.4432 - val_accuracy: 0.8577\n",
      "Epoch 18/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.2150 - accuracy: 0.9320 - val_loss: 0.4746 - val_accuracy: 0.8496\n",
      "Epoch 19/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.1977 - accuracy: 0.9299 - val_loss: 0.4764 - val_accuracy: 0.8577\n",
      "Epoch 20/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.1983 - accuracy: 0.9320 - val_loss: 0.5691 - val_accuracy: 0.8455\n",
      "Epoch 21/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.1873 - accuracy: 0.9392 - val_loss: 0.4552 - val_accuracy: 0.8577\n",
      "Epoch 22/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.1779 - accuracy: 0.9454 - val_loss: 0.5123 - val_accuracy: 0.8333\n",
      "Epoch 23/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.1497 - accuracy: 0.9505 - val_loss: 0.4609 - val_accuracy: 0.8577\n",
      "Epoch 24/40\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.1348 - accuracy: 0.9608 - val_loss: 0.5055 - val_accuracy: 0.8577\n",
      "Epoch 25/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.1184 - accuracy: 0.9629 - val_loss: 0.5758 - val_accuracy: 0.8293\n",
      "Epoch 26/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.1396 - accuracy: 0.9557 - val_loss: 0.4671 - val_accuracy: 0.8577\n",
      "Epoch 27/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.1076 - accuracy: 0.9722 - val_loss: 0.5117 - val_accuracy: 0.8577\n",
      "Epoch 28/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.1066 - accuracy: 0.9660 - val_loss: 0.5157 - val_accuracy: 0.8415\n",
      "Epoch 29/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0918 - accuracy: 0.9742 - val_loss: 0.5262 - val_accuracy: 0.8577\n",
      "Epoch 30/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0828 - accuracy: 0.9763 - val_loss: 0.5448 - val_accuracy: 0.8455\n",
      "Epoch 31/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0640 - accuracy: 0.9856 - val_loss: 0.6810 - val_accuracy: 0.8496\n",
      "Epoch 32/40\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.0687 - accuracy: 0.9814 - val_loss: 0.5358 - val_accuracy: 0.8455\n",
      "Epoch 33/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0671 - accuracy: 0.9784 - val_loss: 0.5807 - val_accuracy: 0.8496\n",
      "Epoch 34/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0664 - accuracy: 0.9814 - val_loss: 0.6216 - val_accuracy: 0.8537\n",
      "Epoch 35/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0527 - accuracy: 0.9907 - val_loss: 0.5712 - val_accuracy: 0.8699\n",
      "Epoch 36/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0579 - accuracy: 0.9835 - val_loss: 0.5595 - val_accuracy: 0.8577\n",
      "Epoch 37/40\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.0332 - accuracy: 0.9969 - val_loss: 0.5455 - val_accuracy: 0.8699\n",
      "Epoch 38/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0365 - accuracy: 0.9907 - val_loss: 0.5466 - val_accuracy: 0.8699\n",
      "Epoch 39/40\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0330 - accuracy: 0.9948 - val_loss: 0.6372 - val_accuracy: 0.8659\n",
      "Epoch 40/40\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.0366 - accuracy: 0.9897 - val_loss: 0.5943 - val_accuracy: 0.8618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x231e38f6b50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate steps per epoch\n",
    "# steps_per_epoch = training_set.samples // training_set.batch_size\n",
    "# print(steps_per_epoch)\n",
    "# validation_steps = test_set.samples // test_set.batch_size\n",
    "# print(validation_steps)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(\n",
    "    training_set,\n",
    "#     steps_per_epoch=50,\n",
    "    epochs=40,\n",
    "    validation_data=test_set\n",
    "#     validation_steps=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06d94a",
   "metadata": {},
   "source": [
    "# Testing our models performance by selecting specific images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0afff5d4-eeb0-46f6-a55d-8cdcab9eebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "#from keras.preprocessing import image\n",
    "test_image = image.load_img('D:/QTInternship/caltech-101/caltech-101/testcheckimages/airplane.jpeg'\n",
    "                   , target_size=(128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56061abf-ef38-4131-bf95-71244cb92209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30aacd67-0064-4a3b-a030-806b6e5a2e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/testcheckimages/butterfly.jpg'\n",
    "                   , target_size=(128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3974e5ff-a483-4fbf-8e82-7c7dd271c1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af7bea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/testcheckimages/cougar_body1.jpg'\n",
    "                   , target_size=(128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "691cea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f55775b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/testcheckimages/cougar_face.jpg'\n",
    "                   , target_size=(128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3ad7b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dfdc95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/testcheckimages/Saltwater_crocodile.jpg'\n",
    "                   , target_size=(128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50879d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "261cdb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/testcheckimages/Crocodilehead.jpg'\n",
    "                   , target_size=(128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37821e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  3.6971433e-29 0.0000000e+00 1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0bd7aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/testcheckimages/dragonfly.jpg'\n",
    "                   , target_size=(128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcbb214b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86e180cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image =image.load_img('D:/QTInternship/caltech-101/caltech-101/testcheckimages/mayfly2.jpg'\n",
    "                   , target_size=(128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "881d2d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a512ff",
   "metadata": {},
   "source": [
    "# Data Labels used for ImageClassification\n",
    "\n",
    "## airplanes\n",
    "## butterfly\n",
    "## cougar_body\n",
    "## cougar_face\n",
    "## crocodile\n",
    "## crocodile_head\n",
    "## dragonfly\n",
    "## mayfly\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
